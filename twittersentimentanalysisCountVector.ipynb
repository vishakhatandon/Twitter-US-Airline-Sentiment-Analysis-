{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "twittersentimentanalysisCountVector.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jF6MZWYQHVH"
      },
      "source": [
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNz2FEBHNeRc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R85s7lz0OwON",
        "outputId": "d86178c9-858b-4f7a-af3b-49bb2d51e546"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAcm-waNalx0"
      },
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/Tweets.csv\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP5V8h7Paypq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data=data[\"text\"]\n",
        "test_data=data[\"airline_sentiment\"]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcRgpjigQHVK"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_data,test_data, test_size=0.30, random_state=42)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vNF_ZV8PyHN",
        "outputId": "40b9cf31-3ff6-4aba-d613-5220b048ccb6"
      },
      "source": [
        "len(x_train)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10248"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI-4U-fcPTrI",
        "outputId": "1fa8fd51-af29-40e7-8b92-557942d70833"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa8VSfbBQHVL"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "documents=[]\n",
        "for text in x_train:\n",
        "    documents.append(word_tokenize(text))\n",
        "    "
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfmxoG9aQHVL",
        "outputId": "902f9397-52be-4a1d-848a-cc058e11a98d"
      },
      "source": [
        "documents2=[]\n",
        "for text in x_test:\n",
        "    documents2.append(word_tokenize(text))\n",
        "documents2[1]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@',\n",
              " 'USAirways',\n",
              " 'how',\n",
              " 'is',\n",
              " 'it',\n",
              " 'that',\n",
              " 'my',\n",
              " 'flt',\n",
              " 'to',\n",
              " 'EWR',\n",
              " 'was',\n",
              " 'Cancelled',\n",
              " 'Flightled',\n",
              " 'yet',\n",
              " 'flts',\n",
              " 'to',\n",
              " 'NYC',\n",
              " 'from',\n",
              " 'USAirways',\n",
              " 'are',\n",
              " 'still',\n",
              " 'flying',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bC1WyWbQTk2",
        "outputId": "d72d6cda-16d4-473a-d452-7d5b40c908c1"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knfUhOQdQHVL"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stop=stopwords.words('english')\n",
        "punctations=list(string.punctuation)\n",
        "stop+=punctations"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wilc1uGfQHVL",
        "outputId": "17f94ae1-4e4a-47b9-c245-1918ca808fed"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lematizer=WordNetLemmatizer()\n",
        "w = \"better\"\n",
        "pos_tag([w])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('better', 'RBR')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXON1T4fQRfy",
        "outputId": "3352c7e5-ed12-4302-921d-d808f4ba3bcf"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txVxoA4HQHVL"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "def get_simple_pos(tag):\n",
        "    \n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  \n",
        "    "
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eabqLZQtQHVL"
      },
      "source": [
        "\n",
        "def clean_text(word):\n",
        "    clean_text=[]\n",
        "    for w in word:\n",
        "        if w.lower() not in stop:\n",
        "            pos=pos_tag([w])\n",
        "            clean_word=lematizer.lemmatize(w,pos=get_simple_pos(pos[0][1]))\n",
        "            clean_text.append(clean_word.lower())\n",
        "    return clean_text"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_00G1BrQHVL"
      },
      "source": [
        "documents=[clean_text(tweet) for tweet in documents]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lg-BdYPQHVL"
      },
      "source": [
        "documents2=[clean_text(tweet) for tweet in documents2 ]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yajvKNZ3QHVL",
        "outputId": "2863f5a7-2767-428d-d173-6911f0cefa57"
      },
      "source": [
        "documents2[5]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['united',\n",
              " 'still',\n",
              " 'wait',\n",
              " 'hear',\n",
              " 'back',\n",
              " 'wallet',\n",
              " 'steal',\n",
              " 'one',\n",
              " 'plane',\n",
              " 'would',\n",
              " 'appreciate',\n",
              " 'resolution']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KPUAwNhDQHVL",
        "outputId": "fcc361e8-df7f-4e24-fef9-27aa52e2ca6e"
      },
      "source": [
        "train_documents=[\" \".join(document) for document in documents]\n",
        "train_documents[2]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"usairways call 12 time last three day 's unacceptable 'm willing wait hold 's option\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G8cAJ_goQHVL",
        "outputId": "538250e7-1f88-4dfa-e315-386b35f74400"
      },
      "source": [
        "test_documents=[\" \".join(document) for document in documents2]\n",
        "test_documents[10]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"usairways 've hold change date ticket 3 hour someone please assist unacceptable\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8fTMwAqQHVL"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect=CountVectorizer(max_features=4000,ngram_range=(1,2))\n",
        "x_train_features=count_vect.fit_transform(train_documents)\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dk8E1rkQHVL"
      },
      "source": [
        "#count_vect.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDVRXECqQHVL"
      },
      "source": [
        "x_test_features=count_vect.transform(test_documents)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRw-49fRlZ0f"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvSsDcVLhfhe"
      },
      "source": [
        "ListOfClassifiers = [\n",
        "LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n",
        "RandomForestClassifier(n_estimators=200),\n",
        "DecisionTreeClassifier(),\n",
        " SVC(kernel=\"rbf\"),\n",
        " MultinomialNB(),\n",
        " GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=4, random_state=0),\n",
        " AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=18, min_samples_leaf=25, min_samples_split=10), n_estimators=10)\n",
        "    ]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1I2bOpJnC2F"
      },
      "source": [
        "advanceFeatures=x_train_features.toarray()\n",
        "advanceFeaturesTest= x_test_features.toarray()"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZcm91P5micS",
        "outputId": "a2f6f357-decb-49c6-fe2d-df90ac202c1c"
      },
      "source": [
        "Accuracy=[]\n",
        "ClassifierModel=[]\n",
        "for classifier in ListOfClassifiers:\n",
        "    try:\n",
        "        fit = classifier.fit(x_train_features,y_train)\n",
        "        pred = fit.predict(x_test_features)\n",
        "    except Exception:\n",
        "        fit = classifier.fit(advanceFeatures,y_train)\n",
        "        pred = fit.predict(advanceFeaturesTest)\n",
        "    accuracy = accuracy_score(pred,y_test)\n",
        "    Accuracy.append(accuracy)\n",
        "    ClassifierModel.append(classifier.__class__.__name__)\n",
        "    print ('Accuracy of '+classifier.__class__.__name__+' is '+str(accuracy))\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of LogisticRegression is 0.6407103825136612\n",
            "Accuracy of RandomForestClassifier is 0.7693533697632058\n",
            "Accuracy of DecisionTreeClassifier is 0.7040072859744991\n",
            "Accuracy of SVC is 0.7855191256830601\n",
            "Accuracy of MultinomialNB is 0.7739071038251366\n",
            "Accuracy of GradientBoostingClassifier is 0.7597905282331512\n",
            "Accuracy of AdaBoostClassifier is 0.7515938069216758\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}